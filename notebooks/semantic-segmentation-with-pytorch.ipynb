{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:58:18.561958Z","iopub.execute_input":"2023-04-25T20:58:18.562355Z","iopub.status.idle":"2023-04-25T20:58:32.627075Z","shell.execute_reply.started":"2023-04-25T20:58:18.562320Z","shell.execute_reply":"2023-04-25T20:58:32.625381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport os\nimport sys\nimport time\nfrom os import listdir\nfrom os.path import isfile\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as F\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models\nfrom torchvision.models.segmentation import FCN_ResNet101_Weights, DeepLabV3_ResNet101_Weights, \\\n    DeepLabV3_MobileNet_V3_Large_Weights\nfrom torchvision.models.segmentation.deeplabv3 import DeepLabHead","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-25T20:58:32.630719Z","iopub.execute_input":"2023-04-25T20:58:32.631130Z","iopub.status.idle":"2023-04-25T20:58:32.640651Z","shell.execute_reply.started":"2023-04-25T20:58:32.631088Z","shell.execute_reply":"2023-04-25T20:58:32.638781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"\\nSegmentation project running on\", device)\n\n# training parameters\ntrain = True\nin_size = (520, 520)\nb_size = 32\n\nlr = 1e-4\nnb_epoch = 6\n\n# model selection\nmodel_choice = \"dlab_large\"\nft = True\nappendix = \"_ft\" if ft else \"\"\n\nif model_choice not in [\"dlab\", \"dlab_large\", \"fcn\"]:\n    print(\"Error (wrong choice) : choose between dlab, dlab_large, or fcn\")\n    sys.exit(1)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-25T20:58:32.642647Z","iopub.execute_input":"2023-04-25T20:58:32.643183Z","iopub.status.idle":"2023-04-25T20:58:32.661995Z","shell.execute_reply.started":"2023-04-25T20:58:32.643140Z","shell.execute_reply":"2023-04-25T20:58:32.660495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CocoDataset(Dataset):\n    def __init__(self, root, subset, transform=None, sup=False):\n        print(f\"\\nLoading {subset} dataset\")\n\n        self.imgs_dir = os.path.join(root + f\"/{subset}2017/\")\n\n        ann_file = os.path.join(\"/kaggle/input/coco-2017-dataset/coco2017/annotations/\", f\"instances_{subset}2017.json\")\n        self.coco = COCO(ann_file)\n\n        self.sup = sup\n        self.classes = self.coco.loadCats(self.coco.getCatIds())\n\n        self.class_names = [cat['name'] for cat in self.classes]\n        self.superclasses = list(set([cat['supercategory'] for cat in self.classes]))\n\n        self.target_classes = self.superclasses if self.sup else self.classes\n\n        self.target_classes_nb = len(self.target_classes) + 1\n\n        self.img_ids = self.coco.getImgIds()\n\n        self.transform = transform\n\n    def assign_class(self, normal_class, attrname):\n        for c in self.classes:\n            if c['id'] == normal_class:\n                return c[attrname]\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        anns = self.coco.loadAnns(self.coco.getAnnIds(img_id))\n        img_obj = self.coco.loadImgs(img_id)[0]\n\n        img = Image.open(os.path.join(self.imgs_dir, img_obj['file_name'])).convert('RGB')\n\n        mask = np.zeros(img.size[::-1], dtype=np.uint8)\n\n        for ann in anns:\n            class_name = self.assign_class(ann['category_id'], 'name')\n            pixel_value = self.class_names.index(class_name) + 1\n            mask = np.maximum(self.coco.annToMask(ann) * pixel_value, mask)\n\n        if self.sup:\n            for cl in self.classes:\n                idx = mask == cl['id']\n                class_index = self.assign_class(cl['id'], 'supercategory')\n                mask[idx] = self.superclasses.index(class_index) + 1\n\n            idx = mask >= self.target_classes_nb\n            mask[idx] = 0\n\n        mask = Image.fromarray(mask)\n\n        if self.transform is not None:\n            img = self.transform(img)\n            img = T.ToTensor()(img)\n            img = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n\n            mask = self.transform(mask)\n            mask = T.PILToTensor()(mask)\n\n        return img, mask.long()\n\n    def __len__(self):\n        return len(self.img_ids)\n\n\nclass CocoTestDataset(Dataset):\n    def __init__(self, root, subset, transform=None):\n        print(f\"\\nLoading {subset} dataset\")\n\n        self.imgs_dir = os.path.join(root + \"/test2017/\")\n        self.img_names = [f for f in listdir(self.imgs_dir) if isfile(os.path.join(self.imgs_dir, f))]\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        img = Image.open(os.path.join(self.imgs_dir, self.img_names[idx])).convert('RGB')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img\n\n    def __len__(self):\n        return len(self.img_names)\n\n\ndef get_data(input_size, batch_size=64, sup=False):\n    data_transforms = {\n        'train': T.Compose([\n            T.Resize(input_size, interpolation=F.InterpolationMode.BILINEAR),\n            T.CenterCrop(input_size)\n        ]),\n        'val': T.Compose([\n            T.Resize(input_size, interpolation=F.InterpolationMode.BILINEAR),\n            T.CenterCrop(input_size),\n        ]),\n        'test': T.Compose([\n            T.Resize(input_size, interpolation=F.InterpolationMode.BILINEAR),\n            T.CenterCrop(input_size),\n            T.ToTensor()\n        ]),\n    }\n\n    coco_train = CocoDataset(root=\"/kaggle/input/coco-2017-dataset/coco2017\", subset=\"train\", transform=data_transforms[\"train\"], sup=sup)\n    sub_train = torch.utils.data.Subset(coco_train, range(0, 15000))\n    train_dl = DataLoader(sub_train, batch_size=batch_size, shuffle=True)\n\n    coco_val = CocoDataset(root=\"/kaggle/input/coco-2017-dataset/coco2017\", subset=\"val\", transform=data_transforms[\"val\"], sup=sup)\n    sub_val = torch.utils.data.Subset(coco_val, range(0, 1200))\n    val_dl = DataLoader(sub_val, batch_size=batch_size, shuffle=True)\n\n    coco_test = CocoTestDataset(root=\"/kaggle/input/coco-2017-dataset/coco2017\", subset=\"test\", transform=data_transforms[\"test\"])\n    sub_test = torch.utils.data.Subset(coco_test, range(0, 150))\n    test_dl = DataLoader(sub_test, batch_size=None, shuffle=True)\n\n    cats = ['unlabeled'] + coco_train.target_classes\n\n    return train_dl, val_dl, test_dl, cats\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-25T20:58:32.664124Z","iopub.execute_input":"2023-04-25T20:58:32.664543Z","iopub.status.idle":"2023-04-25T20:58:32.697010Z","shell.execute_reply.started":"2023-04-25T20:58:32.664503Z","shell.execute_reply":"2023-04-25T20:58:32.695537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_segmap(image, colormap, nc):\n    r = np.zeros_like(image).astype(np.uint8)\n    g = np.zeros_like(image).astype(np.uint8)\n    b = np.zeros_like(image).astype(np.uint8)\n\n    for l in range(0, nc):\n        idx = image == l\n        r[idx] = colormap[l][0]\n        g[idx] = colormap[l][1]\n        b[idx] = colormap[l][2]\n\n    rgb = np.stack([r, g, b], axis=2)\n\n    return rgb\n\n\ndef image_overlay(image, segmented_image):\n    alpha = 1  # transparency for the original image\n    beta = 0.75  # transparency for the segmentation map\n    gamma = 0  # scalar added to each sum\n\n    image = np.array(image)\n\n    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    return image\n\n\ndef detect_classes(img, cats, nb_class):\n    detected = []\n    for lp in range(0, nb_class):\n        idx = img == lp\n\n        if idx.any():\n            detected.append(lp)\n\n    return [cats[cnb] for cnb in detected]\n\n\ndef segment_map(output, img, colormap, cats, nb_class):\n    om = torch.argmax(output.squeeze(), dim=0).detach().cpu().numpy()\n\n    cnames = detect_classes(om, cats, nb_class)\n\n    segmented_image = decode_segmap(om, colormap, nb_class)\n\n    # Resize to original image size\n    segmented_image = cv2.resize(segmented_image, om.shape, cv2.INTER_CUBIC)\n\n    np_img = np.array(img * 255, dtype=np.uint8)\n\n    overlayed_image = image_overlay(np_img, segmented_image)\n\n    return segmented_image, overlayed_image, cnames\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-25T20:58:32.700256Z","iopub.execute_input":"2023-04-25T20:58:32.701414Z","iopub.status.idle":"2023-04-25T20:58:32.716394Z","shell.execute_reply.started":"2023-04-25T20:58:32.701366Z","shell.execute_reply":"2023-04-25T20:58:32.715126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(choice=\"dlab\", train=False, feat_extract=False, nb_class=1):\n    print()\n\n    if choice == \"dlab\":\n        print(f\"Model is {choice}\")\n        w = DeepLabV3_ResNet101_Weights.DEFAULT\n        m = models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True, weights=w)\n\n    elif choice == \"dlab_large\":\n        print(f\"Model is {choice}\")\n        w = DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT\n        m = models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True, progress=True, weights=w)\n    elif choice == \"fcn\":\n        print(f\"Model is FCN\")\n        w = FCN_ResNet101_Weights.DEFAULT\n        m = models.segmentation.fcn_resnet101(pretrained=True, progress=True, weights=w)\n    else:\n        return\n\n    m.aux_classifier = None\n\n    if train:\n        m = create_trainable_dlab(m, nb_class)\n\n    if feat_extract:\n        for param in m.parameters():\n            param.requires_grad = False\n\n        for param in m.classifier.parameters():\n            param.requires_grad = True\n\n    params = [param for (name, param) in m.named_parameters() if param.requires_grad]\n\n    return m, params\n\n\ndef create_trainable_dlab(model, nb_class):\n    sample_input = torch.randn(1, 3, 32, 32)  # batch size 1, RGB input image of size 520x520\n    backbone_output = model.backbone(sample_input)['out']  # get output of backbone module\n    prev_channels = backbone_output.shape[1]\n    model.classifier = DeepLabHead(prev_channels, nb_class)\n\n    return model\n\n\ndef inference(model, dataloader, cats, nb_class, device, nbinf=5):\n    model = model.eval()\n\n    palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n    colors = torch.as_tensor([i for i in range(nb_class)])[:, None] * palette\n    colormap = (colors % 255).numpy().astype(\"uint8\")\n\n    with torch.no_grad():\n        for i, img in enumerate(dataloader):\n            print(\"Iteration %d\" % i)\n            inp = img.unsqueeze(0).to(device)\n            inp = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(inp)\n\n            st = time.time()\n            out = model.to(device)(inp)['out']\n            end = time.time()\n\n            print(f\"Inference took: {end - st:.2f}\", )\n\n            f_img = img.permute(1, 2, 0)\n            seg, overlay, cnames = segment_map(out, f_img, colormap, cats, nb_class)\n\n            plot_results(f_img, seg, overlay, cnames)\n\n            if i == nbinf:\n                break\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-25T20:58:32.718035Z","iopub.execute_input":"2023-04-25T20:58:32.718370Z","iopub.status.idle":"2023-04-25T20:58:32.736991Z","shell.execute_reply.started":"2023-04-25T20:58:32.718337Z","shell.execute_reply":"2023-04-25T20:58:32.735519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_all(history):\n    plot_metric(history, \"acc\")\n    plot_metric(history, \"loss\")\n    plot_metric(history, \"score\")\n\n\ndef plot_metric(data, lb):\n    plt.plot(data[lb]['train'], label=f'train_{lb}', marker='o')\n    plt.plot(data[lb]['val'], label=f'val_{lb}', marker='o')\n    plt.title(f'{lb} per epoch')\n    plt.ylabel(lb)\n    plt.xlabel('epoch')\n    plt.legend(), plt.grid()\n    plt.show()\n\n\ndef plot_results(img, segmented_image, overlayed_image, cn):\n    # Create the figure and subplots\n    fig, axs = plt.subplots(1, 4, figsize=(12, 5), dpi=100)\n\n    axs[0].axis(\"off\")\n    axs[0].set_title(\"Image\")\n    axs[0].imshow(img)\n\n    axs[1].set_title(\"Segmentation\")\n    axs[1].axis(\"off\")\n    axs[1].imshow(segmented_image)\n\n    axs[2].set_title(\"Overlayed\")\n    axs[2].axis(\"off\")\n    axs[2].imshow(overlayed_image)\n\n    axs[3].set_title(\"Detected objects\")\n    axs[3].axis(\"off\")\n    axs[3].text(0, 0, '\\n'.join(cn))\n\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n\n    plt.show()\n\n\ndef get_classes(fpath):\n    # Initialize dictionary\n    label_dict = {}\n\n    # Open file and read each line\n    with open(fpath, 'r') as f:\n        for line in f:\n            line = line.strip()\n            label_id, label_name = line.split(': ')\n\n            label_dict[int(label_id)] = label_name\n\n    return label_dict\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-25T20:58:32.738976Z","iopub.execute_input":"2023-04-25T20:58:32.739465Z","iopub.status.idle":"2023-04-25T20:58:32.757334Z","shell.execute_reply.started":"2023-04-25T20:58:32.739412Z","shell.execute_reply":"2023-04-25T20:58:32.755839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _fast_hist(label_true, label_pred, n_class):\n    label_true = label_true.numpy()\n    label_pred = label_pred.numpy()\n\n    mask = (label_true >= 0) & (label_true < n_class)\n    hist = np.bincount(\n        n_class * label_true[mask].astype(int) + label_pred[mask],\n        minlength=n_class ** 2,\n    ).reshape(n_class, n_class)\n    return hist\n\n\ndef metrics_report(label_trues, label_preds, n_class):\n    hist = np.zeros((n_class, n_class))\n\n    for lt, lp in zip(label_trues, label_preds):\n        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n\n    acc = np.diag(hist).sum() / hist.sum()\n\n    acc_cls = np.diag(hist) / hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n\n    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n\n    valid = hist.sum(axis=1) > 0  # added\n\n    mean_iu = np.nanmean(iu[valid])\n\n    freq = hist.sum(axis=1) / hist.sum()\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n\n    cls_iu = dict(zip(range(n_class), iu))\n\n    return {\n        \"pixel accuracy\": acc,\n        \"mean accuracy\": acc_cls,\n        \"frequency weighted IoU\": fwavacc,\n        \"mean IoU\": mean_iu,\n        \"class IoU\": cls_iu,\n    }\n\n\ndef pixel_accuracy(output, mask):\n    correct = torch.eq(output, mask).int()\n\n    accuracy = correct.sum(dim=(1, 2, 3)) / (mask.shape[2] * mask.shape[3])\n    mean_accuracy = accuracy.mean()\n\n    return mean_accuracy.item()\n\n\ndef mIoU(pred_mask, mask, n_classes, smooth=1e-10):\n    pred_mask = pred_mask.view(-1)\n    mask = mask.view(-1)\n\n    iou_per_class = []\n    for clas in range(n_classes):\n        true_class = pred_mask == clas\n        true_label = mask == clas\n\n        intersect = (true_class & true_label).float().sum()\n        union = (true_class | true_label).float().sum()\n\n        if union == 0:\n            iou_per_class.append(np.nan)\n        else:\n            iou_val = (intersect + smooth) / (union + smooth)\n            iou_per_class.append(iou_val.item())\n\n    return np.nanmean(iou_per_class)\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-25T20:58:32.759903Z","iopub.execute_input":"2023-04-25T20:58:32.760443Z","iopub.status.idle":"2023-04-25T20:58:32.779951Z","shell.execute_reply.started":"2023-04-25T20:58:32.760390Z","shell.execute_reply":"2023-04-25T20:58:32.778499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, nb_class, device, epochs=15):\n    model = model.to(device)\n\n    since = time.time()\n\n    train_acc_history = []\n    train_loss_history = []\n    train_score_history = []\n\n    val_acc_history = []\n    val_loss_history = []\n    val_score_history = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(epochs):\n        print('Epoch {}/{}'.format(epoch + 1, epochs))\n        print('-' * 20)\n        \n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()  # Set model to evaluate mode\n\n            running_loss = 0\n            running_acc = 0\n            running_miou = 0\n            i = 0\n\n            # Iterate over data.\n            for images, masks in dataloaders[phase]:\n                \n                if (i % 50) == 0:\n                    print(\"batch number\", i)\n                \n                images = images.to(device)\n                masks = masks.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n\n                    outputs = model(images)['out']\n\n                    loss = criterion(outputs, masks.squeeze(1))\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                preds = torch.argmax(outputs, dim=1).unsqueeze(1).float()\n\n                # statistics\n                running_loss += loss.item() * images.size(0)\n                running_acc += pixel_accuracy(preds, masks)\n                running_miou += mIoU(preds, masks, nb_class)\n                i += 1\n\n            epoch_loss = running_loss / len(dataloaders[phase])\n            epoch_acc = running_acc / len(dataloaders[phase]) * 100\n            epoch_miou = running_miou / len(dataloaders[phase]) * 100\n\n            print('{} loss: {:.4f} acc: {:.2f}% mIoU {:.2f}%'.format(phase, epoch_loss, epoch_acc, epoch_miou))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n                val_loss_history.append(epoch_loss)\n                val_score_history.append(epoch_miou)\n            else:\n                train_acc_history.append(epoch_acc)\n                train_loss_history.append(epoch_loss)\n                train_score_history.append(epoch_miou)\n\n    time_elapsed = time.time() - since\n    print('\\nTraining complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val accuracy: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n\n    metrics = {\n        \"acc\": {\"train\": train_acc_history, \"val\": val_acc_history},\n        \"loss\": {\"train\": train_loss_history, \"val\": val_loss_history},\n        \"score\": {\"train\": train_score_history, \"val\": val_score_history}\n    }\n\n    return model, metrics","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-25T20:58:32.782092Z","iopub.execute_input":"2023-04-25T20:58:32.783154Z","iopub.status.idle":"2023-04-25T20:58:32.803068Z","shell.execute_reply.started":"2023-04-25T20:58:32.783098Z","shell.execute_reply":"2023-04-25T20:58:32.801655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train:\n\n    train_ds, val_ds, test_ds, cats = get_data(input_size=in_size, batch_size=b_size, sup=True)\n    nb_classes = len(cats)\n    model, params_to_update = load_model(choice=model_choice, train=train, feat_extract=ft, nb_class=nb_classes)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(params_to_update, lr=lr)\n\n    dls = {\"train\": train_ds, \"val\": val_ds}","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:58:32.804795Z","iopub.execute_input":"2023-04-25T20:58:32.805239Z","iopub.status.idle":"2023-04-25T21:01:45.893396Z","shell.execute_reply.started":"2023-04-25T20:58:32.805192Z","shell.execute_reply":"2023-04-25T21:01:45.892233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train:\n    model, history = train_model(model, dls, criterion, optimizer, nb_classes, device, epochs=nb_epoch)\n\n    torch.save(model, f\"{model_choice}{appendix}.pt\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-25T21:01:45.895888Z","iopub.execute_input":"2023-04-25T21:01:45.896287Z","iopub.status.idle":"2023-04-25T21:10:07.550449Z","shell.execute_reply.started":"2023-04-25T21:01:45.896246Z","shell.execute_reply":"2023-04-25T21:10:07.548520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train:\n     plot_all(history)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T22:23:48.381568Z","iopub.execute_input":"2023-04-24T22:23:48.382314Z","iopub.status.idle":"2023-04-24T22:23:49.083010Z","shell.execute_reply.started":"2023-04-24T22:23:48.382272Z","shell.execute_reply":"2023-04-24T22:23:49.081914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if True:\n\n    # _, _, test_ds, cats = get_data(input_size=in_size, batch_size=b_size, sup=True)\n\n    # load trained model if available\n    m_path = f\"{model_choice}{appendix}.pt\"\n\n    if os.path.exists(m_path):\n        print(\"Model file found, using pretrained model for inference\\n\")\n        nb_classes = len(cats)\n        model = torch.load(m_path)\n    else:\n        print(\"Model file not found, using Pytorch's model for inference\\n\")\n        cats = get_classes(\"pascal.txt\")\n        nb_classes = len(cats)\n        model, _ = load_model(choice=model_choice)\n\n    inference(model, test_ds, cats, nb_classes, device, nbinf=5)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T22:25:19.125041Z","iopub.execute_input":"2023-04-24T22:25:19.125787Z","iopub.status.idle":"2023-04-24T22:25:43.598726Z","shell.execute_reply.started":"2023-04-24T22:25:19.125741Z","shell.execute_reply":"2023-04-24T22:25:43.597769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\nEnd of the program\")","metadata":{"execution":{"iopub.status.busy":"2023-04-24T22:21:01.061104Z","iopub.status.idle":"2023-04-24T22:21:01.061591Z","shell.execute_reply.started":"2023-04-24T22:21:01.061336Z","shell.execute_reply":"2023-04-24T22:21:01.061361Z"},"trusted":true},"execution_count":null,"outputs":[]}]}