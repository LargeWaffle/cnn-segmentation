{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models\n",
    "from torchvision.models.segmentation import FCN_ResNet101_Weights, DeepLabV3_ResNet101_Weights, \\\n",
    "    DeepLabV3_MobileNet_V3_Large_Weights\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"\\nSegmentation project running on\", device)\n",
    "\n",
    "# training parameters\n",
    "train = False\n",
    "in_size = (520, 520)\n",
    "b_size = 2\n",
    "\n",
    "# model selection\n",
    "model_choice = \"dlab\"\n",
    "ft = True\n",
    "appendix = \"_ft\" if ft else \"\"\n",
    "\n",
    "if model_choice not in [\"dlab\", \"dlab_large\", \"fcn\"]:\n",
    "    print(\"Error (wrong choice) : choose between dlab, dlab_large, or fcn\")\n",
    "    sys.exit(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, subset, transform=None, sup=False):\n",
    "        print(f\"\\nLoading {subset} dataset\")\n",
    "\n",
    "        self.imgs_dir = os.path.join(root + f\"/{subset}2017/\", subset)\n",
    "\n",
    "        ann_file = os.path.join(\"/kaggle/input/coco-2017-dataset/coco2017/annotation/\", f\"instances_{subset}2017.json\")\n",
    "        self.coco = COCO(ann_file)\n",
    "\n",
    "        self.sup = sup\n",
    "        self.classes = self.coco.loadCats(self.coco.getCatIds())\n",
    "\n",
    "        self.class_names = [cat['name'] for cat in self.classes]\n",
    "        self.superclasses = list(set([cat['supercategory'] for cat in self.classes]))\n",
    "\n",
    "        self.target_classes = self.superclasses if self.sup else self.classes\n",
    "\n",
    "        self.target_classes_nb = len(self.target_classes) + 1\n",
    "\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def assign_class(self, normal_class, attrname):\n",
    "        for c in self.classes:\n",
    "            if c['id'] == normal_class:\n",
    "                return c[attrname]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        anns = self.coco.loadAnns(self.coco.getAnnIds(img_id))\n",
    "        img_obj = self.coco.loadImgs(img_id)[0]\n",
    "\n",
    "        img = Image.open(os.path.join(self.imgs_dir, img_obj['file_name'])).convert('RGB')\n",
    "\n",
    "        mask = np.zeros(img.size[::-1], dtype=np.uint8)\n",
    "\n",
    "        for ann in anns:\n",
    "            class_name = self.assign_class(ann['category_id'], 'name')\n",
    "            pixel_value = self.class_names.index(class_name) + 1\n",
    "            mask = np.maximum(self.coco.annToMask(ann) * pixel_value, mask)\n",
    "\n",
    "        if self.sup:\n",
    "            for cl in self.classes:\n",
    "                idx = mask == cl['id']\n",
    "                class_index = self.assign_class(cl['id'], 'supercategory')\n",
    "                mask[idx] = self.superclasses.index(class_index) + 1\n",
    "\n",
    "            idx = mask >= self.target_classes_nb\n",
    "            mask[idx] = 0\n",
    "\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            img = T.ToTensor()(img)\n",
    "            img = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
    "\n",
    "            mask = self.transform(mask)\n",
    "            mask = T.PILToTensor()(mask)\n",
    "\n",
    "        return img, mask.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "\n",
    "class CocoTestDataset(Dataset):\n",
    "    def __init__(self, root, subset, transform=None):\n",
    "        print(f\"\\nLoading {subset} dataset\")\n",
    "\n",
    "        self.imgs_dir = os.path.join(root + \"/test2017/\", subset)\n",
    "        self.img_names = [f for f in listdir(self.imgs_dir) if isfile(os.path.join(self.imgs_dir, f))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.imgs_dir, self.img_names[idx])).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "\n",
    "def get_data(input_size, batch_size=64, sup=False):\n",
    "    data_transforms = {\n",
    "        'train': T.Compose([\n",
    "            T.Resize(input_size, interpolation=F.InterpolationMode.BILINEAR),\n",
    "            T.CenterCrop(input_size)\n",
    "        ]),\n",
    "        'val': T.Compose([\n",
    "            T.Resize(input_size, interpolation=F.InterpolationMode.BILINEAR),\n",
    "            T.CenterCrop(input_size),\n",
    "        ]),\n",
    "        'test': T.Compose([\n",
    "            T.Resize(input_size, interpolation=F.InterpolationMode.BILINEAR),\n",
    "            T.CenterCrop(input_size),\n",
    "            T.ToTensor()\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    coco_train = CocoDataset(root=\"/kaggle/input/coco-2017-dataset/coco2017\", subset=\"train\", transform=data_transforms[\"train\"], sup=sup)\n",
    "    sub1 = torch.utils.data.Subset(coco_train, range(0, 10))\n",
    "\n",
    "    train_dl = DataLoader(sub1, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    coco_val = CocoDataset(root=\"/kaggle/input/coco-2017-dataset/coco2017\", subset=\"val\", transform=data_transforms[\"val\"], sup=sup)\n",
    "    sub2 = torch.utils.data.Subset(coco_val, range(0, 5))\n",
    "\n",
    "    val_dl = DataLoader(sub2, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    coco_test = CocoTestDataset(root=\"/kaggle/input/coco-2017-dataset/coco2017\", subset=\"test\", transform=data_transforms[\"test\"])\n",
    "    sub3 = torch.utils.data.Subset(coco_test, range(0, 10))\n",
    "\n",
    "    test_dl = DataLoader(sub3, batch_size=None, shuffle=True)\n",
    "\n",
    "    cats = ['unlabeled'] + coco_train.target_classes\n",
    "\n",
    "    return train_dl, val_dl, test_dl, cats\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def decode_segmap(image, colormap, nc):\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "\n",
    "    for l in range(0, nc):\n",
    "        idx = image == l\n",
    "        r[idx] = colormap[l][0]\n",
    "        g[idx] = colormap[l][1]\n",
    "        b[idx] = colormap[l][2]\n",
    "\n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def image_overlay(image, segmented_image):\n",
    "    alpha = 1  # transparency for the original image\n",
    "    beta = 0.75  # transparency for the segmentation map\n",
    "    gamma = 0  # scalar added to each sum\n",
    "\n",
    "    image = np.array(image)\n",
    "\n",
    "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def detect_classes(img, cats, nb_class):\n",
    "    detected = []\n",
    "    for lp in range(0, nb_class):\n",
    "        idx = img == lp\n",
    "\n",
    "        if idx.any():\n",
    "            detected.append(lp)\n",
    "\n",
    "    return [cats[cnb] for cnb in detected]\n",
    "\n",
    "\n",
    "def segment_map(output, img, colormap, cats, nb_class):\n",
    "    om = torch.argmax(output.squeeze(), dim=0).detach().cpu().numpy()\n",
    "\n",
    "    cnames = detect_classes(om, cats, nb_class)\n",
    "\n",
    "    segmented_image = decode_segmap(om, colormap, nb_class)\n",
    "\n",
    "    # Resize to original image size\n",
    "    segmented_image = cv2.resize(segmented_image, om.shape, cv2.INTER_CUBIC)\n",
    "\n",
    "    np_img = np.array(img * 255, dtype=np.uint8)\n",
    "\n",
    "    overlayed_image = image_overlay(np_img, segmented_image)\n",
    "\n",
    "    return segmented_image, overlayed_image, cnames\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_model(choice=\"dlab\", train=False, feat_extract=False, nb_class=1):\n",
    "    print()\n",
    "\n",
    "    if choice == \"dlab\":\n",
    "        print(f\"Model is {choice}\")\n",
    "        w = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "        m = models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True, weights=w)\n",
    "\n",
    "    elif choice == \"dlab_large\":\n",
    "        print(f\"Model is {choice}\")\n",
    "        w = DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT\n",
    "        m = models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True, progress=True, weights=w)\n",
    "    elif choice == \"fcn\":\n",
    "        print(f\"Model is FCN\")\n",
    "        w = FCN_ResNet101_Weights.DEFAULT\n",
    "        m = models.segmentation.fcn_resnet101(pretrained=True, progress=True, weights=w)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    m.aux_classifier = None\n",
    "\n",
    "    if train:\n",
    "        m = create_trainable_dlab(m, nb_class)\n",
    "\n",
    "    if feat_extract:\n",
    "        for param in m.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in m.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    params = [param for (name, param) in m.named_parameters() if param.requires_grad]\n",
    "\n",
    "    return m, params\n",
    "\n",
    "\n",
    "def create_trainable_dlab(model, nb_class):\n",
    "    sample_input = torch.randn(1, 3, 32, 32)  # batch size 1, RGB input image of size 520x520\n",
    "    backbone_output = model.backbone(sample_input)['out']  # get output of backbone module\n",
    "    prev_channels = backbone_output.shape[1]\n",
    "    model.classifier = DeepLabHead(prev_channels, nb_class)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def inference(model, dataloader, cats, nb_class, device, nbinf=5):\n",
    "    model = model.eval()\n",
    "\n",
    "    palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
    "    colors = torch.as_tensor([i for i in range(nb_class)])[:, None] * palette\n",
    "    colormap = (colors % 255).numpy().astype(\"uint8\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, img in enumerate(dataloader):\n",
    "            print(\"Iteration %d\" % i)\n",
    "            inp = img.unsqueeze(0).to(device)\n",
    "            inp = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(inp)\n",
    "\n",
    "            st = time.time()\n",
    "            out = model.to(device)(inp)['out']\n",
    "            end = time.time()\n",
    "\n",
    "            print(f\"Inference took: {end - st:.2f}\", )\n",
    "\n",
    "            f_img = img.permute(1, 2, 0)\n",
    "            seg, overlay, cnames = segment_map(out, f_img, colormap, cats, nb_class)\n",
    "\n",
    "            plot_results(f_img, seg, overlay, cnames)\n",
    "\n",
    "            if i == nbinf:\n",
    "                break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_all(history):\n",
    "    plot_metric(history, \"acc\")\n",
    "    plot_metric(history, \"loss\")\n",
    "    plot_metric(history, \"score\")\n",
    "\n",
    "\n",
    "def plot_metric(data, lb):\n",
    "    plt.plot(data[lb]['train'], label=f'train_{lb}', marker='o')\n",
    "    plt.plot(data[lb]['val'], label=f'val_{lb}', marker='o')\n",
    "    plt.title(f'{lb} per epoch')\n",
    "    plt.ylabel(lb)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_results(img, segmented_image, overlayed_image, cn):\n",
    "    # Create the figure and subplots\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 5), dpi=100)\n",
    "\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[0].set_title(\"Image\")\n",
    "    axs[0].imshow(img)\n",
    "\n",
    "    axs[1].set_title(\"Segmentation\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[1].imshow(segmented_image)\n",
    "\n",
    "    axs[2].set_title(\"Overlayed\")\n",
    "    axs[2].axis(\"off\")\n",
    "    axs[2].imshow(overlayed_image)\n",
    "\n",
    "    axs[3].set_title(\"Detected objects\")\n",
    "    axs[3].axis(\"off\")\n",
    "    axs[3].text(0, 0, '\\n'.join(cn))\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_classes(fpath):\n",
    "    # Initialize dictionary\n",
    "    label_dict = {}\n",
    "\n",
    "    # Open file and read each line\n",
    "    with open(fpath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            label_id, label_name = line.split(': ')\n",
    "\n",
    "            label_dict[int(label_id)] = label_name\n",
    "\n",
    "    return label_dict\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    label_true = label_true.numpy()\n",
    "    label_pred = label_pred.numpy()\n",
    "\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) + label_pred[mask],\n",
    "        minlength=n_class ** 2,\n",
    "    ).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def metrics_report(label_trues, label_preds, n_class):\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "\n",
    "    acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "\n",
    "    valid = hist.sum(axis=1) > 0  # added\n",
    "\n",
    "    mean_iu = np.nanmean(iu[valid])\n",
    "\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "\n",
    "    cls_iu = dict(zip(range(n_class), iu))\n",
    "\n",
    "    return {\n",
    "        \"pixel accuracy\": acc,\n",
    "        \"mean accuracy\": acc_cls,\n",
    "        \"frequency weighted IoU\": fwavacc,\n",
    "        \"mean IoU\": mean_iu,\n",
    "        \"class IoU\": cls_iu,\n",
    "    }\n",
    "\n",
    "\n",
    "def pixel_accuracy(output, mask):\n",
    "    correct = torch.eq(output, mask).int()\n",
    "\n",
    "    accuracy = correct.sum(dim=(1, 2, 3)) / (mask.shape[2] * mask.shape[3])\n",
    "    mean_accuracy = accuracy.mean()\n",
    "\n",
    "    return mean_accuracy.item()\n",
    "\n",
    "\n",
    "def mIoU(pred_mask, mask, n_classes, smooth=1e-10):\n",
    "    pred_mask = pred_mask.view(-1)\n",
    "    mask = mask.view(-1)\n",
    "\n",
    "    iou_per_class = []\n",
    "    for clas in range(n_classes):\n",
    "        true_class = pred_mask == clas\n",
    "        true_label = mask == clas\n",
    "\n",
    "        intersect = (true_class & true_label).float().sum()\n",
    "        union = (true_class | true_label).float().sum()\n",
    "\n",
    "        if union == 0:\n",
    "            iou_per_class.append(np.nan)\n",
    "        else:\n",
    "            iou_val = (intersect + smooth) / (union + smooth)\n",
    "            iou_per_class.append(iou_val)\n",
    "\n",
    "    return np.nanmean(iou_per_class)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, nb_class, device, epochs=15):\n",
    "    model = model.to(device)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "    train_score_history = []\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_score_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "            running_miou = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for images, masks in dataloaders[phase]:\n",
    "\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    outputs = model(images)['out']\n",
    "\n",
    "                    loss = criterion(outputs, masks.squeeze(1))\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1).unsqueeze(1).float()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                running_acc += pixel_accuracy(preds, masks)\n",
    "                running_miou += mIoU(preds, masks, nb_class)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase])\n",
    "            epoch_acc = running_acc / len(dataloaders[phase]) * 100\n",
    "            epoch_miou = running_miou / len(dataloaders[phase]) * 100\n",
    "\n",
    "            print('{} loss: {:.4f} acc: {:.2f}% mIoU {:.2f}%'.format(phase, epoch_loss, epoch_acc, epoch_miou))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_score_history.append(epoch_miou)\n",
    "            else:\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_score_history.append(epoch_miou)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('\\nTraining complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val accuracy: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    metrics = {\n",
    "        \"acc\": {\"train\": train_acc_history, \"val\": val_acc_history},\n",
    "        \"loss\": {\"train\": train_loss_history, \"val\": val_loss_history},\n",
    "        \"score\": {\"train\": train_score_history, \"val\": val_score_history}\n",
    "    }\n",
    "\n",
    "    return model, metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if train:\n",
    "\n",
    "    train_ds, val_ds, test_ds, cats = get_data(input_size=in_size, batch_size=b_size, sup=True)\n",
    "    nb_classes = len(cats)\n",
    "\n",
    "    model, params_to_update = load_model(choice=model_choice, train=train, feat_extract=ft, nb_class=nb_classes)\n",
    "\n",
    "    lr = 1e-4\n",
    "    nb_epoch = 3\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(params_to_update, lr=lr)\n",
    "\n",
    "    dls = {\"train\": train_ds, \"val\": val_ds}\n",
    "\n",
    "    model, history = train_model(model, dls, criterion, optimizer, nb_classes, device, epochs=nb_epoch)\n",
    "\n",
    "    torch.save(model, f\"{model_choice}/{model_choice}{appendix}.pt\")\n",
    "\n",
    "    plot_all(history)\n",
    "else:\n",
    "\n",
    "    _, _, test_ds, cats = get_data(input_size=in_size, batch_size=b_size, sup=True)\n",
    "\n",
    "    # load trained model if available\n",
    "    m_path = f\"{model_choice}/{model_choice}{appendix}.pt\"\n",
    "\n",
    "    if os.path.exists(m_path):\n",
    "        print(\"Model file found, using pretrained model for inference\\n\")\n",
    "        nb_classes = len(cats)\n",
    "        model = torch.load(m_path)\n",
    "    else:\n",
    "        print(\"Model file not found, using Pytorch's model for inference\\n\")\n",
    "        cats = get_classes(\"pascal.txt\")\n",
    "        nb_classes = len(cats)\n",
    "        model, _ = load_model(choice=model_choice)\n",
    "\n",
    "    inference(model, test_ds, cats, nb_classes, device, nbinf=5)\n",
    "\n",
    "print(\"\\nEnd of the program\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
